{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df243a4f",
   "metadata": {},
   "source": [
    "## Project Goals\n",
    "The goal of this project is to assess features of real estate to predict tax value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667eee9b",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "With low interest rates and a strong buyer's market, it is increasingly important to identify valuable real estate investment opportunities. I will observe various details of properties within a few counties of California and will use the information to create a model to estimate the tax value and provide a recommendation in improving model predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d955b9",
   "metadata": {},
   "source": [
    "## Initial Questions\n",
    "\n",
    "1. Does square footage affect tax value?\n",
    "2. Do more bedrooms or bathrooms have a higher tax value?\n",
    "3. Do newer houses have a higher tax value than older houses?\n",
    "4. Does location affect tax value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5f7518",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb610e35",
   "metadata": {},
   "source": [
    "| variable      | meaning       |\n",
    "| ------------- |:-------------:|\n",
    "|logerror|target value: log(zestimate) - log(sale price)\n",
    "|lm|Ordinary Least Squares Linear Regression modeling algorithm|\n",
    "|lm2|Polynomial Regression modeling algorithm |\n",
    "|lars|Lasso + Lars Regression modeling algorithm|\n",
    "|glm|TweedieRegressor modeling algorithm|\n",
    "|df|Dataframe of raw zillow data from sql server|\n",
    "|train| training dataset, a major cut from the df|\n",
    "|validate| validate dataset, used to prevent overfitting|\n",
    "|test| test dataset, to test the top model on unseen data|\n",
    "|pearsonr| statistical test used to compare churn with various categories|\n",
    "|taxvaluedollarcnt| The assessed value of the built structure on the parcel|\n",
    "|calculatedfinishedsquarefeet| Calculated total finished living area of the home |\n",
    "|sqft|abbreviation for square feet|\n",
    "|structuretaxvaluedollarcnt| assessed value of the structure|\n",
    "|landtaxvaluedollarcnt|assessed value of the land|\n",
    "|taxamount|total property tax assessed for that year|\n",
    "|bedroomcnt| Number of bedrooms in home |\n",
    "|bathroomcnt| Number of bathrooms in home including fractional bathrooms|\n",
    "|latitude/longitude| coordinates of the property|\n",
    "|acres| lotsizesquarefeet / 43560|\n",
    "|age| 2017 - yearbuilt|\n",
    "|dollar_per_acre| landtaxvaluedollarcnt / acres|\n",
    "|dollar_per_sqft| structuretaxvaluedollarcnt / calculatedfinishedsquarefeet|\n",
    "|features_cluster|cluster created with features of the property|\n",
    "|value_cluster|cluster created with dollar features|\n",
    "|development_cluster|cluster created with location and age data|\n",
    "|fips| County codes for property locations|\n",
    "| County Codes||\n",
    "|6037 | Los Angeles, CA|\n",
    "|6059 | Orange, CA|\n",
    "|6111 | Ventura, CA|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb30df6",
   "metadata": {},
   "source": [
    "## Wrangle Zillow Data\n",
    "To acquire the zillow data used, I ran a query on the zillow database from the mySQL server. I queried all columns from the zillow database by using multiple joins. I excluded any properties that did not have coordinates included or a transaction in 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43a0092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "\n",
    "# import visualizations\n",
    "# import viz\n",
    "\n",
    "# ignore warnings to reduce clutter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import wrangle module with functions to acquire, prepare, scale, and split data from SQL server's zillow database\n",
    "import wrangle\n",
    "\n",
    "# execute functions to acquire and split a df and store in train, validate, and test dataframes\n",
    "train, validate, test = wrangle.split_data(wrangle.wrangle_zillow())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0384115",
   "metadata": {},
   "source": [
    "Cleaning the data was executed in the following steps:\n",
    "- Filter data to single family homes\n",
    "- Drop duplicates\n",
    "- Removed outliers by setting parameters for features\n",
    "- Create new columns to reduce total feature number\n",
    "- Drop unnecessary columns\n",
    "- Drop any reamining null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a46653",
   "metadata": {},
   "source": [
    "## Our Data Landscape\n",
    "- Properties that had transactions in 2017.\n",
    "- 46,763 properties assessed after data cleaning.\n",
    "- Located throughout three California counties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d5c5cef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize the number of properties in each county after data cleaning.\n",
    "# viz.data_landscape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ea7f0",
   "metadata": {},
   "source": [
    "# Clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38f878",
   "metadata": {},
   "source": [
    "Prior to clustering, we scaled our data using a min/max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4be5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "scaler, train_scaled, validate_scaled, test_scaled = wrangle.min_max_scaler(train, validate, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a975107e",
   "metadata": {},
   "source": [
    "Clusters were created by grouping features realtive to the property\n",
    "- features_cluster: bed/bath/sqft\n",
    "- value_cluster: tax value features\n",
    "- development_cluster: location and age features\n",
    "\n",
    "The number of clusters (k) was found by using the elbow method, except for the development_cluster, which was was given a higher k for a better estimation of housing subdivisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c221d",
   "metadata": {},
   "source": [
    "## Cluster 1: features_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36d030e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import clusters\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# define features for clustering\n",
    "X_train_feature_cluster = train_scaled[['bedroomcnt','bathroomcnt','calculatedfinishedsquarefeet','taxvaluedollarcnt']]\n",
    "\n",
    "#repeat for validate and test\n",
    "X_validate_feature_cluster = validate_scaled[['bedroomcnt','bathroomcnt','calculatedfinishedsquarefeet','taxvaluedollarcnt']]\n",
    "X_test_feature_cluster = test_scaled[['bedroomcnt','bathroomcnt','calculatedfinishedsquarefeet','taxvaluedollarcnt']]\n",
    "\n",
    "# define cluster object\n",
    "kmeans = KMeans(n_clusters=5, random_state = 333)\n",
    "# fit cluster object to features\n",
    "kmeans.fit(X_train_feature_cluster)\n",
    "# use the object\n",
    "train_scaled['feature_cluster'] = kmeans.predict(X_train_feature_cluster)\n",
    "validate_scaled['feature_cluster'] = kmeans.predict(X_validate_feature_cluster)\n",
    "test_scaled['feature_cluster'] = kmeans.predict(X_test_feature_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "177534c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### visualize ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544d3b71",
   "metadata": {},
   "source": [
    "## Cluster 2: value_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2376faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features for clustering\n",
    "X_train_value_cluster = train_scaled[['structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxamount']]\n",
    "\n",
    "#repeat for validate and test\n",
    "X_validate_value_cluster = validate_scaled[['structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxamount']]\n",
    "X_test_value_cluster = test_scaled[['structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxamount']]\n",
    "\n",
    "# define cluster object\n",
    "kmeans = KMeans(n_clusters=4, random_state = 333)\n",
    "# fit cluster object to features\n",
    "kmeans.fit(X_train_value_cluster)\n",
    "# use the object\n",
    "train_scaled['value_cluster'] = kmeans.predict(X_train_value_cluster)\n",
    "validate_scaled['value_cluster'] = kmeans.predict(X_validate_value_cluster)\n",
    "test_scaled['value_cluster'] = kmeans.predict(X_test_value_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "501cda64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualize ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78f4cce",
   "metadata": {},
   "source": [
    "## Cluster 3: development_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d0ce395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features for clustering\n",
    "X_train_development_cluster = train[['latitude','longitude','age']]\n",
    "X_validate_development_cluster = validate[['latitude','longitude','age']]\n",
    "X_test_development_cluster = test[['latitude','longitude','age']]\n",
    "\n",
    "# define cluster object\n",
    "kmeans = KMeans(n_clusters=10, random_state = 333)\n",
    "# fit cluster object to features\n",
    "kmeans.fit(X_train_development_cluster)\n",
    "# use the object\n",
    "train_scaled['development_cluster'] = kmeans.predict(X_train_development_cluster)\n",
    "validate_scaled['development_cluster'] = kmeans.predict(X_validate_development_cluster)\n",
    "test_scaled['development_cluster'] = kmeans.predict(X_test_development_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2a4513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualize ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77daa21",
   "metadata": {},
   "source": [
    "## Exploratory analysis: What correlates with log error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91affce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statistics module and establish alpha\n",
    "from scipy import stats\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b5c3c",
   "metadata": {},
   "source": [
    "### 1. Is logerror different between clusters based on house features?\n",
    "- Run anova test\n",
    "\n",
    "$H_{0}$: Means of the logerror between clusters are equal\n",
    "\n",
    "$H_{a}$ Means of the logerror between clusters are not equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3bfeb77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean logerror of the clusters are not equal.\n"
     ]
    }
   ],
   "source": [
    "f, p = stats.f_oneway(train_scaled[train_scaled.feature_cluster == 0].logerror,\n",
    "                     train_scaled[train_scaled.feature_cluster == 1].logerror,\n",
    "                     train_scaled[train_scaled.feature_cluster == 2].logerror,\n",
    "                     train_scaled[train_scaled.feature_cluster == 3].logerror)\n",
    "\n",
    "if p < alpha:\n",
    "    print(\"The mean logerror of the clusters are not equal.\")\n",
    "else:\n",
    "    print(\"The mean logerror of the clusters are equal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5707a63c",
   "metadata": {},
   "source": [
    "#### The means are not equal so we check for correlation\n",
    "- pearsonr test\n",
    "\n",
    "$H_{0}$: There is no correlation between feature clusters and logerror\n",
    "\n",
    "$H_{a}$ There is a correlation between feature clusters and logerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f254a391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no correlation between feature clusters and logerror\n"
     ]
    }
   ],
   "source": [
    "x = train_scaled.feature_cluster\n",
    "y = train_scaled.logerror\n",
    "\n",
    "corr, p = stats.pearsonr(x, y)\n",
    "\n",
    "if p < alpha:\n",
    "    print('There is a correlation between feature clusters and logerror')\n",
    "else:\n",
    "    print('There is no correlation between feature clusters and logerror')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d502051",
   "metadata": {},
   "source": [
    "#### We can use this cluster for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db1011",
   "metadata": {},
   "source": [
    "### 2. Is logerror different between clusters based on value and is there a correlation between the clusters and logerror?\n",
    "\n",
    "- Run anova test\n",
    "\n",
    "$H_{0}$: Means of the logerror between clusters are equal\n",
    "\n",
    "$H_{a}$ Means of the logerror between clusters are not equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc185b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean logerror of the clusters are not equal.\n"
     ]
    }
   ],
   "source": [
    "f, p = stats.f_oneway(train_scaled[train_scaled.value_cluster == 0].logerror,\n",
    "                     train_scaled[train_scaled.value_cluster == 1].logerror,\n",
    "                     train_scaled[train_scaled.value_cluster == 2].logerror,\n",
    "                     train_scaled[train_scaled.value_cluster == 3].logerror)\n",
    "\n",
    "if p < alpha:\n",
    "    print(\"The mean logerror of the clusters are not equal.\")\n",
    "else:\n",
    "    print(\"The mean logerror of the clusters are equal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aacf99",
   "metadata": {},
   "source": [
    "#### The means are different, is there a correlation?\n",
    "\n",
    "- pearsonr test\n",
    "\n",
    "$H_{0}$: There is no correlation between feature clusters and logerror\n",
    "\n",
    "$H_{a}$ There is a correlation between feature clusters and logerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6c786ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no correlation between value clusters and logerror\n"
     ]
    }
   ],
   "source": [
    "x = train_scaled.value_cluster\n",
    "y = train_scaled.logerror\n",
    "\n",
    "corr, p = stats.pearsonr(x, y)\n",
    "\n",
    "if p < alpha:\n",
    "    print('There is a correlation between value clusters and logerror')\n",
    "else:\n",
    "    print('There is no correlation between value clusters and logerror')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a824dd",
   "metadata": {},
   "source": [
    "#### We can use this cluster for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901afff3",
   "metadata": {},
   "source": [
    "### 3. Is logerror different between clusters based on location and age and is there a correlation between the clusters and logerror?\n",
    "\n",
    "- Run anova test\n",
    "\n",
    "$H_{0}$: Means of the logerror between clusters are equal\n",
    "\n",
    "$H_{a}$ Means of the logerror between clusters are not equal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3213ade7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean logerror of the clusters are not equal.\n"
     ]
    }
   ],
   "source": [
    "f, p = stats.f_oneway(train_scaled[train_scaled.development_cluster == 0].logerror,\n",
    "                     train_scaled[train_scaled.development_cluster == 1].logerror,\n",
    "                     train_scaled[train_scaled.development_cluster == 2].logerror,\n",
    "                     train_scaled[train_scaled.development_cluster == 3].logerror)\n",
    "\n",
    "if p < alpha:\n",
    "    print(\"The mean logerror of the clusters are not equal.\")\n",
    "else:\n",
    "    print(\"The mean logerror of the clusters are equal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90397727",
   "metadata": {},
   "source": [
    "#### The means are different, is there a correlation?\n",
    "\n",
    "- pearsonr test\n",
    "\n",
    "$H_{0}$: There is no correlation between feature clusters and logerror\n",
    "\n",
    "$H_{a}$ There is a correlation between feature clusters and logerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b281f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a correlation between development clusters and logerror\n"
     ]
    }
   ],
   "source": [
    "x = train_scaled.development_cluster\n",
    "y = train_scaled.logerror\n",
    "\n",
    "corr, p = stats.pearsonr(x, y)\n",
    "\n",
    "if p < alpha:\n",
    "    print('There is a correlation between development clusters and logerror')\n",
    "else:\n",
    "    print('There is no correlation between development clusters and logerror')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8f2927",
   "metadata": {},
   "source": [
    "#### We will be using this cluster for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be86b98",
   "metadata": {},
   "source": [
    "### 4. Use SelectKBest to learn which features have the strongest relationship with logerror, including our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "219932dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calculatedfinishedsquarefeet',\n",
       " 'fips',\n",
       " 'structuretaxvaluedollarcnt',\n",
       " 'taxvaluedollarcnt',\n",
       " 'landtaxvaluedollarcnt',\n",
       " 'taxamount',\n",
       " 'age',\n",
       " 'dollar_per_sqft',\n",
       " 'dollar_per_acre',\n",
       " 'development_cluster']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Define our X_train, minus some of the features we've included in our clusters and our target\n",
    "X_train_scaled = train_scaled.drop(columns=['logerror','latitude','longitude','bedroomcnt','bathroomcnt'])\n",
    "y_train = train_scaled.logerror\n",
    "\n",
    "\n",
    "# our selecter is an f_regression stats test and retrieving 10 features\n",
    "f_selector = SelectKBest(f_regression, k=10)\n",
    "\n",
    "# find the top 5 features correlated with our target\n",
    "f_selector.fit(X_train_scaled, y_train)\n",
    "\n",
    "# boolean mask of whether the column was selected or not. \n",
    "feature_mask = f_selector.get_support()\n",
    "\n",
    "# get list of top K features. \n",
    "f_feature = X_train_scaled.iloc[:,feature_mask].columns.tolist()\n",
    "\n",
    "f_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013bdc49",
   "metadata": {},
   "source": [
    "We will model with SelectKBest's recommendation in addition to our selected clusters.\n",
    "\n",
    "Statistical tests for the SelectKBest's features were performed in our exploration notebook, available for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725e24ea",
   "metadata": {},
   "source": [
    "### 5. What is the correlation between taxvaluedollarcnt and logerror?\n",
    "\n",
    "- pearsonr test\n",
    "\n",
    "$H_{0}$: There is no correlation between taxvaluedollarcnt and logerror\n",
    "\n",
    "$H_{a}$ There is a correlation between taxvaluedollarcnt and logerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f4363da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a correlation between taxvaluedollarcnt and logerror\n"
     ]
    }
   ],
   "source": [
    "x = train.taxvaluedollarcnt\n",
    "y = train.logerror\n",
    "\n",
    "corr, p = stats.pearsonr(x, y)\n",
    "\n",
    "if p < alpha:\n",
    "    print('There is a correlation between taxvaluedollarcnt and logerror')\n",
    "else:\n",
    "    print('There is no correlation between taxvaluedollarcnt and logerror')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b687fb8b",
   "metadata": {},
   "source": [
    "### 6. What is the correlation between landtaxvaluedollarcnt and logerror?\n",
    "\n",
    "- pearsonr test\n",
    "\n",
    "$H_{0}$: There is no correlation between landtaxvaluedollarcnt and logerror\n",
    "\n",
    "$H_{a}$ There is a correlation between landtaxvaluedollarcnt and logerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "055be761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a correlation between landtaxvaluedollarcnt and logerror\n"
     ]
    }
   ],
   "source": [
    "x = train.landtaxvaluedollarcnt\n",
    "y = train.logerror\n",
    "\n",
    "corr, p = stats.pearsonr(x, y)\n",
    "\n",
    "if p < alpha:\n",
    "    print('There is a correlation between landtaxvaluedollarcnt and logerror')\n",
    "else:\n",
    "    print('There is no correlation between landtaxvaluedollarcnt and logerror')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2eaeee",
   "metadata": {},
   "source": [
    "### 7. What is the correlation between taxamount and logerror?\n",
    "\n",
    "- pearsonr test\n",
    "\n",
    "$H_{0}$: There is no correlation between taxamount and logerror\n",
    "\n",
    "$H_{a}$ There is a correlation between taxamount and logerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b253cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a correlation between taxamount and logerror\n"
     ]
    }
   ],
   "source": [
    "x = train.taxamount\n",
    "y = train.logerror\n",
    "\n",
    "corr, p = stats.pearsonr(x, y)\n",
    "\n",
    "if p < alpha:\n",
    "    print('There is a correlation between taxamount and logerror')\n",
    "else:\n",
    "    print('There is no correlation between taxamount and logerror')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac1a476",
   "metadata": {},
   "source": [
    "### 8. What is the correlation between dollar_per_sqft and logerror?\n",
    "\n",
    "- pearsonr test\n",
    "\n",
    "$H_{0}$: There is no correlation between dollar_per_sqft and logerror\n",
    "\n",
    "$H_{a}$ There is a correlation between dollar_per_sqft and logerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6445b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a correlation between dollar_per_sqft and logerror\n"
     ]
    }
   ],
   "source": [
    "x = train.dollar_per_sqft\n",
    "y = train.logerror\n",
    "\n",
    "corr, p = stats.pearsonr(x, y)\n",
    "\n",
    "if p < alpha:\n",
    "    print('There is a correlation between dollar_per_sqft and logerror')\n",
    "else:\n",
    "    print('There is no correlation between dollar_per_sqft and logerror')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6870e02",
   "metadata": {},
   "source": [
    "### 9. What is the correlation between dollar_per_acre and logerror?\n",
    "\n",
    "- pearsonr test\n",
    "\n",
    "$H_{0}$: There is no correlation between dollar_per_acre and logerror\n",
    "\n",
    "$H_{a}$ There is a correlation between dollar_per_acre and logerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "796a5530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a correlation between dollar_per_acre and logerror\n"
     ]
    }
   ],
   "source": [
    "x = train.dollar_per_acre\n",
    "y = train.logerror\n",
    "\n",
    "corr, p = stats.pearsonr(x, y)\n",
    "\n",
    "if p < alpha:\n",
    "    print('There is a correlation between dollar_per_acre and logerror')\n",
    "else:\n",
    "    print('There is no correlation between dollar_per_acre and logerror')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc80c3",
   "metadata": {},
   "source": [
    "## So what affects logerror?\n",
    "Through statistical tests I've determined the development cluster is not useful and that will not be included in modeling. Our other clusters and the features selected by SelectKBest are each correlated with logerror and we'll use these moving forward in modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d04ea2",
   "metadata": {},
   "source": [
    "## Modeling: Predicting logerror\n",
    "I am using linear regression algorithms to predict a continous target variable. The models will be fit on a training dataset and then validated with a separate dataset to ensure there is no overfitting. The primary measure of model performance I will be using is RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b3aaa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import linear regression modules\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#import evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b8605",
   "metadata": {},
   "source": [
    "### Create X and y for modeling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00a427b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features to use in modeling\n",
    "features = ['taxvaluedollarcnt','landtaxvaluedollarcnt','taxamount','dollar_per_sqft','dollar_per_acre','feature_cluster','development_cluster','fips','age']\n",
    "\n",
    "# X_train with selected features and clusters\n",
    "X_train = train_scaled[features]\n",
    "# Series containing target\n",
    "y_train = train.logerror\n",
    "\n",
    "# X and y validate are created\n",
    "X_validate = validate_scaled[features]\n",
    "y_validate = validate.logerror\n",
    "\n",
    "# X and y test are created\n",
    "X_test = test_scaled[features]\n",
    "y_test = test.logerror\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba6f0a1",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "A baseline is created by evaluating the mean logerror for all properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c729c28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/In-Sample RMSE:  0.1659611321947964\n",
      "Validate/Out-of-Sample RMSE:  0.15719086355809453\n"
     ]
    }
   ],
   "source": [
    "# Create dataframes of the target for modeling purposes\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_validate = pd.DataFrame(y_validate)\n",
    "\n",
    "# predict mean\n",
    "pred_mean = y_train.logerror.mean()\n",
    "y_train['baseline_pred_mean'] = pred_mean\n",
    "y_validate['baseline_pred_mean'] = pred_mean\n",
    "\n",
    "# RMSE of mean\n",
    "rmse_train = mean_squared_error(y_train.logerror, y_train.baseline_pred_mean)**(1/2)\n",
    "print(\"Train/In-Sample RMSE: \", rmse_train)\n",
    "\n",
    "# validate rmse\n",
    "rmse_validate = mean_squared_error(y_validate.logerror, y_validate.baseline_pred_mean)**(1/2)\n",
    "print(\"Validate/Out-of-Sample RMSE: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f325acb9",
   "metadata": {},
   "source": [
    "## Four Models\n",
    "The models I created were Ordinary Least Squares, TweedieRegressor, LassoLars, and Polynomial linear regression models. The best performing model was the OLS model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6029ba",
   "metadata": {},
   "source": [
    "## OLS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a990550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/In-Sample RMSE:  0.1656713845011223\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "lm = LinearRegression(normalize=True)\n",
    "\n",
    "# fit the model to training data.\n",
    "lm.fit(X_train, y_train.logerror)\n",
    "\n",
    "# predict train\n",
    "y_train['logerror_pred_lm'] = lm.predict(X_train)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.logerror, y_train.logerror_pred_lm)**(1/2)\n",
    "\n",
    "print(\"Training/In-Sample RMSE: \", rmse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd1d317",
   "metadata": {},
   "source": [
    "### Evaluate on Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53f825c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Out-of-Sample RMSE: 0.15690202014524396\n"
     ]
    }
   ],
   "source": [
    "# predict validate\n",
    "y_validate['logerror_pred_lm'] = lm.predict(X_validate)\n",
    "\n",
    "# evaluate rmse\n",
    "rmse_validate = mean_squared_error(y_validate.logerror, y_validate.logerror_pred_lm)**(1/2)\n",
    "\n",
    "print(\"Validation/Out-of-Sample RMSE:\", rmse_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9168eb",
   "metadata": {},
   "source": [
    "## GLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73dcf062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for GLM using Tweedie, power=0 & alpha=0\n",
      "Training/In-Sample:  0.16594032397004282\n"
     ]
    }
   ],
   "source": [
    "# create the model object\n",
    "glm = TweedieRegressor(power = 0, alpha=0.5)\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "glm.fit(X_train, y_train.logerror)\n",
    "\n",
    "# predict train\n",
    "y_train['logerror_pred_glm'] = glm.predict(X_train)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.logerror, y_train.logerror_pred_glm)**(1/2)\n",
    "\n",
    "print(\"RMSE for GLM using Tweedie, power=0 & alpha=0\\nTraining/In-Sample: \", rmse_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec0650f",
   "metadata": {},
   "source": [
    "### Evaluate on Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0368aeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Out-of-Sample:  0.15717636856413858\n"
     ]
    }
   ],
   "source": [
    "# predict validate\n",
    "y_validate['logerror_pred_glm'] = glm.predict(X_validate)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate.logerror, y_validate.logerror_pred_glm)**(1/2)\n",
    "\n",
    "print(\"Validation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6893c0e",
   "metadata": {},
   "source": [
    "## Polynomial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6eac4b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Polynomial Model, degrees=2\n",
      "Training/In-Sample:  0.165429264457974\n"
     ]
    }
   ],
   "source": [
    "# make the polynomial features to get a new set of features\n",
    "pf = PolynomialFeatures(degree=2)\n",
    "\n",
    "# fit and transform X_train\n",
    "X_train_degree2 = pf.fit_transform(X_train)\n",
    "\n",
    "# transform X_validate & X_test\n",
    "X_validate_degree2 = pf.transform(X_validate)\n",
    "X_test_degree2 = pf.transform(X_test)\n",
    "\n",
    "# create the model object\n",
    "lm2 = LinearRegression(normalize=True)\n",
    "\n",
    "# fit the model to our training data.\n",
    "lm2.fit(X_train_degree2, y_train.logerror)\n",
    "\n",
    "# predict train\n",
    "y_train['logerror_pred_lm2'] = lm2.predict(X_train_degree2)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.logerror, y_train.logerror_pred_lm2)**(1/2)\n",
    "\n",
    "print(\"RMSE for Polynomial Model, degrees=2\\nTraining/In-Sample: \", rmse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b339ce",
   "metadata": {},
   "source": [
    "### Evaluate on Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77e9b295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Out-of-Sample: 0.15684287809918773\n"
     ]
    }
   ],
   "source": [
    "# predict validate\n",
    "y_validate['logerror_pred_lm2'] = lm2.predict(X_validate_degree2)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate.logerror, y_validate.logerror_pred_lm2)**(1/2)\n",
    "\n",
    "print('Validation/Out-of-Sample:', rmse_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e3966",
   "metadata": {},
   "source": [
    "## LassoLars Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6991d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for LassoLars Model, alpha = 10 \n",
      "Training/In-Sample:  0.1659611321947964\n"
     ]
    }
   ],
   "source": [
    "# create the model object\n",
    "lars = LassoLars(alpha=10)\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "lars.fit(X_train, y_train.logerror)\n",
    "\n",
    "# predict train\n",
    "y_train['logerror_pred_lars'] = lars.predict(X_train)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.logerror, y_train.logerror_pred_lars)**(1/2)\n",
    "\n",
    "\n",
    "print(\"RMSE for LassoLars Model, alpha = 10 \\nTraining/In-Sample: \", rmse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8288a5c9",
   "metadata": {},
   "source": [
    "### Evaluate on Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6fac4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Out-of-Sample:  0.15719086355809453\n"
     ]
    }
   ],
   "source": [
    "# predict validate\n",
    "y_validate['logerror_pred_lars'] = lars.predict(X_validate)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate.logerror, y_validate.logerror_pred_lars)**(1/2)\n",
    "\n",
    "print('Validation/Out-of-Sample: ', rmse_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede2430b",
   "metadata": {},
   "source": [
    "## Validating with Visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b7c1e",
   "metadata": {},
   "source": [
    "A scatter plot of each models' predictions vs the actual tax values. The ideal line for my predictions is plotted for reference. There is a lot of overlap, but there is a slightly better fit for the polynomial model, displayed in yellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e703b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz.validate_scatter(y_validate, pred_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5fb840",
   "metadata": {},
   "source": [
    "Similarly to the scatterplot, the polynomial model's histogram compared to the other models' is slightly more fit to the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e34d6d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz.validate_hist(y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d4d98",
   "metadata": {},
   "source": [
    "## Evaluate on Test\n",
    "The polynomial model performed the best of the three models and will be evaluated on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66173dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-Sample Performance RMSE:  0.1779674436457955\n"
     ]
    }
   ],
   "source": [
    "y_test = pd.DataFrame(y_test)\n",
    "\n",
    "# predict on test\n",
    "y_test['logerror_pred_lars'] = lm.predict(X_test)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_test = mean_squared_error(y_test.logerror, y_test.logerror_pred_lars)**(1/2)\n",
    "\n",
    "print(\"Out-of-Sample Performance RMSE: \", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91707835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz.test_hist(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e849e2dc",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### Summary\n",
    "#### What features matter?\n",
    "Through statistical testing of the features I analyzed, I've identified that the square footage weighs heaviest on the tax value than the other features.\n",
    "\n",
    "#### Modeling\n",
    "The data was modeled through three different linear regression algorithms, with the Polynomial Regression model being the top performer. The model performed as well as it had in training in the validation data and slightly better in the test dataset.\n",
    "\n",
    "### Recommendations\n",
    "My recommendation to improve predictions is to use more refined models that explore more features of the properties, aiming to become a more accurate model.\n",
    "\n",
    "### Next Steps\n",
    "To improve modeling, retreiving more data of the properties from the database to explore and assess whether they would be beneficial to include in modeling. I would also like to spend more time running statistical tests to verify relationships of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631a7ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
